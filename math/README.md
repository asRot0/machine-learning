# ğŸ“š Machine Learning & Deep Learning Math

This folder contains the **mathematical foundations of ML & AI**, with clear implementations in Python. Below, we visualize and explain essential formulas used in **optimization, activation functions, loss calculations, and more**.

---

## ğŸ”¢ 1ï¸âƒ£ Gradient Descent (Optimization Algorithm)

### **Formula:**
$$
\LARGE \theta_{t+1} = \theta_t - \eta \nabla J(\theta_t)
$$

- $`\theta_{t+1}`$ â†’ Updated parameter.
- $`\theta_t`$ â†’ Model parameters at step \( t \).  
- $`\eta`$ â†’ Learning rate, controlling step size.  
- $`\nabla J(\theta_t)`$ â†’ Gradient of the cost function $J(\theta)$ with respect to $\theta\$ at time step $\t$.  

ğŸ“Œ **Why It Matters?**
- Adjusts weights in ML/DL models to **minimize the loss function**.  
- The backbone of training algorithms in **neural networks, linear regression, etc.**  

ğŸ”— **Python Implementation:** [`gradient_descent.py`](gradient_descent.py)

---

## ğŸ§  2ï¸âƒ£ Softmax Function (Classification)

### **Formula:**
$$
\LARGE \text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}
$$

- $`z_i`$ â†’ Raw model output (logits).  
- $`e^{z_i}`$ â†’ Exponential transformation ensuring positive values.  
- $`\sum e^{z_j}`$ â†’ Normalization factor ensuring probabilities sum to 1.  

ğŸ“Œ **Why It Matters?**
- Converts raw model outputs into **probabilities**.  
- Used in **multi-class classification (e.g., MNIST digit classification)**.  

ğŸ”— **Python Implementation:** [`softmax.py`](softmax.py)

---

## ğŸ¯ 3ï¸âƒ£ Cross-Entropy Loss (For Classification Models)

### **Formula:**
$$
\LARGE \mathcal{L} = -\sum_{i=1}^{n} y_i \log(\hat{y_i})
$$

- **\( y_i \)** â†’ True class label (ground truth).  
- **\( \hat{y_i} \)** â†’ Predicted probability for class \( i \).  

ğŸ“Œ **Why It Matters?**
- Measures how well predicted probabilities match true labels.  
- Common in **logistic regression, CNNs, NLP models**.  

ğŸ”— **Python Implementation:** [`cross_entropy.py`](cross_entropy.py)

---

## ğŸ”„ 4ï¸âƒ£ Adam Optimizer (Advanced Gradient Descent)

### **Formula:**
$$
\LARGE \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{v_t} + \epsilon} m_t
$$

- **\( \theta_t \)** â†’ Model parameters at step \( t \).  
- **\( m_t \)** â†’ First moment estimate (mean of gradients).  
- **\( v_t \)** â†’ Second moment estimate (variance of gradients).  
- **\( \eta \)** â†’ Learning rate.  
- **\( \epsilon \)** â†’ Small constant to avoid division by zero.  

ğŸ“Œ **Why It Matters?**
- Adaptive learning rate method, used in **CNNs, RNNs, Transformers**.  
- Faster and more stable than standard **gradient descent**.  

ğŸ”— **Python Implementation:** [`adam_optimizer.py`](adam_optimizer.py)

---

## ğŸ“ˆ 5ï¸âƒ£ Convolution Operation (Feature Extraction in CNNs)

### **Formula:**
$$
\LARGE O(i, j) = \sum_m \sum_n I(i+m, j+n) \cdot K(m, n)
$$

- **\( O(i, j) \)** â†’ Output feature map at position \( i, j \).  
- **\( I(i+m, j+n) \)** â†’ Input image pixel values affected by the filter.  
- **\( K(m, n) \)** â†’ Kernel (filter) values.  

ğŸ“Œ **Why It Matters?**
- Core operation in **image processing & CNNs**.  
- Detects edges, patterns, and textures in **computer vision**.  

ğŸ”— **Python Implementation:** [`convolution.py`](convolution.py)

---

## ğŸ¤– 6ï¸âƒ£ Transformer Attention (NLP Models like BERT, GPT)

### **Formula:**
$$
\LARGE \text{Attention}(Q, K, V) = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right) V
$$

- **\( Q, K, V \)** â†’ Query, Key, and Value matrices.  
- **\( d_k \)** â†’ Dimensionality of key vectors (scaling factor).  
- **\( QK^T \)** â†’ Dot product of queries and keys to compute attention scores.  
- **\( \text{softmax} \)** â†’ Normalization to ensure values sum to 1.  

ğŸ“Œ **Why It Matters?**
- Used in **transformers (GPT, BERT, T5)** for NLP.  
- Enables models to **focus on relevant words** in a sentence.  

ğŸ”— **Python Implementation:** [`transformer_attention.py`](transformer_attention.py)

---

## ğŸ“Œ Summary
This folder contains **core mathematical operations used in ML & AI**. Each `.py` file provides:  
âœ… **A clean Python implementation**  
âœ… **Well-commented, easy-to-understand code**  
âœ… **Example usage for machine learning models**  
